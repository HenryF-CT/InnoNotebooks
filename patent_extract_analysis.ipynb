{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f97b1b-c32c-493c-b1c9-96cbeafe7db7",
   "metadata": {},
   "source": [
    "# Patent data extraction analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077adab4-5d50-4a2a-b974-29f6804a0465",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to test and investigate the different methods to extract the SDG-relevant data from the patents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b86527b-650b-4bd3-9872-75b23015a1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=d5c2ba240646037f456abe63124a9d845f1d2ca766f5d55fee33ce5057db478d\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/5e/90/99/807a5ad861ce5d22c3c299a11df8cba9f31524f23ae6e645cb\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f332474e-58fc-4742-8c24-a36d9b944c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from epo.tipdata.epab import EPABClient\n",
    "epab = EPABClient(env=\"TEST\")\n",
    "import re\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import (\n",
    "    HTML, IntText, Text, Dropdown, SelectMultiple, Checkbox, Button, VBox, HBox, Layout\n",
    ")\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "import json\n",
    "from autocorrect import Speller #TODO check if we do the spelling stuff here\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300fbde6-9fc4-46a5-9c78-1e45cf0d3c46",
   "metadata": {},
   "source": [
    "Patents data loading\n",
    "\n",
    "First we test here different methods to load representative sets of patents for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621fced6-7132-46f1-8b3d-d1e65df20d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f216735252454ec1a70d9735951bcb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='', description='Logs:', layout=Layout(height='200px', width='100%'), placeholde…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the log display area\n",
    "log_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Logs will appear here.',\n",
    "    description='Logs:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%', height='200px')  # Adjust size as needed\n",
    ")\n",
    "\n",
    "# Create a VBox to hold the log display\n",
    "log_widget = widgets.VBox([log_output])\n",
    "\n",
    "# Display the log widget\n",
    "display(log_widget)\n",
    "\n",
    "def log(text):\n",
    "    \"\"\"\n",
    "    Adds text to the log display widget.\n",
    "    \"\"\"\n",
    "    log_output.value += text + '\\n'  # Append the new text with a newline character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95073316-bb1b-48c8-a9e2-f4b47dda34da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to load and save lists of dictionaries to a gzipped jsonl FILE\n",
    "#-----------------------------\n",
    "def save_list(data, filename):\n",
    "    \"\"\"\n",
    "    Saves a list of dictionaries to a gzipped JSON Lines (jsonl) file.\n",
    "\n",
    "    Parameters:\n",
    "        data (list): List of dictionaries to be saved.\n",
    "        filename (str): The filename (including .gz if desired) where the data will be saved.\n",
    "    \"\"\"\n",
    "    with gzip.open(filename, 'wt', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            # Write each dictionary as a JSON line.\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "def load_list(filename):\n",
    "    \"\"\"\n",
    "    Loads a gzipped JSON Lines (jsonl) file and returns a list of dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The filename of the gzipped jsonl file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries read from the file.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            result.append(json.loads(line))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9662e-5cfb-4f97-b7e5-688fd5fd8197",
   "metadata": {},
   "source": [
    "## REQUESTING PATENTS FROM EPO DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b204be6-4227-49be-95fc-574322803c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the save_data function to save the request parameters for further use\n",
    "def save_data(params):\n",
    "    # Create a configuration file with the filename from params\n",
    "    conf_filename = f\"{params['filename']}.conf\"\n",
    "    with open(conf_filename, \"w\") as f:\n",
    "        json.dump(params, f, indent=4)\n",
    "    log(f\"File saved to {conf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd7e06c5-836b-46e0-a736-11cd2403c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_request_random(params):\n",
    "    num_files=params[\"nbr\"]\n",
    "    nbrextract=params[\"nbrextract\"]\n",
    "    statement = f\"\"\"\n",
    "          SELECT epab_doc_id, LEFT(description.text, {nbrextract}) as description, publication.date as pubdate, publication.number as pubnbr\n",
    "        FROM `{epab.full_table_name}`\n",
    "        WHERE description.language=\"EN\"\n",
    "        ORDER BY RAND()\n",
    "        LIMIT {num_files};\"\"\"\n",
    "    return statement\n",
    "\n",
    "def create_sql_request_ipc(params):\n",
    "    num_files=params[\"nbr\"]\n",
    "    nbrextract=params[\"nbrextract\"]\n",
    "    query=f\"\"\"WITH flattened AS (\n",
    "      SELECT\n",
    "        epab_doc_id, \n",
    "        ipc_struct.symbol AS ipc_class\n",
    "      FROM `{epab.full_table_name}`,\n",
    "           UNNEST(ipc) AS ipc_struct\n",
    "    ),\n",
    "    stratified_sample AS (\n",
    "      SELECT\n",
    "        ipc_class,\n",
    "        epab_doc_id,\n",
    "        LEFT(description.text, {nbrextract}) as description,\n",
    "        publication.date as pubdate, publication.number as pubnbr,\n",
    "        ROW_NUMBER() OVER(PARTITION BY ipc_class ORDER BY RAND() LIMIT {num_files}) AS rn\n",
    "      FROM flattened\n",
    "    )\n",
    "    SELECT\n",
    "      ipc_class,\n",
    "      epab_doc_id,\n",
    "      pubdate, pubnbr, description,\n",
    "    FROM stratified_sample\n",
    "    WHERE rn = 1;\"\"\"\n",
    "    return query\n",
    "\n",
    "def create_sql_request_date(params):\n",
    "    nbrextract=params[\"nbrextract\"]\n",
    "    num_files=params[\"nbr\"]\n",
    "    statement = f\"\"\"\n",
    "   DECLARE num_files INT64 DEFAULT {num_files};\n",
    "\n",
    "WITH DateRange AS (\n",
    "  SELECT\n",
    "    MIN(TIMESTAMP(PARSE_DATE('%Y%m%d', publication.date))) AS min_timestamp,\n",
    "    CURRENT_TIMESTAMP() AS max_timestamp\n",
    "  FROM\n",
    "    `{epab.full_table_name}`\n",
    "  WHERE publication.date IS NOT NULL -- Handle nulls\n",
    "),\n",
    "DateBuckets AS (\n",
    "  SELECT\n",
    "    TIMESTAMP(DATE_ADD(DATE(DateRange.min_timestamp), INTERVAL CAST(offset * (DATE_DIFF(DateRange.max_timestamp, DateRange.min_timestamp, DAY) / 10) AS INT64) DAY)) AS bucket_start,\n",
    "    TIMESTAMP(DATE_ADD(DATE(DateRange.min_timestamp), INTERVAL CAST((offset + 1) * (DATE_DIFF(DateRange.max_timestamp, DateRange.min_timestamp, DAY) / 10) AS INT64) DAY)) AS bucket_end\n",
    "  FROM\n",
    "    DateRange,\n",
    "    UNNEST(GENERATE_ARRAY(0, 9)) AS offset\n",
    "),\n",
    "GroupedData AS (\n",
    "  SELECT\n",
    "    epab_doc_id,\n",
    "    LEFT(description.text, {nbrextract}) as description, publication.number as pubnbr,\n",
    "    publication.date as pubdate,\n",
    "    TIMESTAMP(PARSE_DATE('%Y%m%d', publication.date)) as publication_timestamp,\n",
    "    bucket_start,\n",
    "    bucket_end\n",
    "  FROM\n",
    "    `{epab.full_table_name}`\n",
    "  JOIN\n",
    "    DateBuckets\n",
    "  ON\n",
    "    TIMESTAMP(PARSE_DATE('%Y%m%d', publication.date)) >= bucket_start AND TIMESTAMP(PARSE_DATE('%Y%m%d', publication.date)) < bucket_end\n",
    "  WHERE description.language=\"EN\" AND publication.date IS NOT NULL\n",
    "),\n",
    "SampledData AS (\n",
    "  SELECT\n",
    "    epab_doc_id,\n",
    "    description,\n",
    "    pubdate, pubnbr,\n",
    "    bucket_start\n",
    "  FROM\n",
    "    GroupedData\n",
    "  QUALIFY ROW_NUMBER() OVER (PARTITION BY bucket_start ORDER BY RAND()) <= (num_files / 10)\n",
    ")\n",
    "SELECT\n",
    "  epab_doc_id,\n",
    "  description,\n",
    "  pubdate, pubnbr\n",
    "FROM\n",
    "  SampledData;\"\"\"\n",
    "    return statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73598d-ae34-4518-8bd7-1513703fdf69",
   "metadata": {},
   "source": [
    "Enter here the filename to save raw patent descriptions requested from the database. The purpose is to avoid permanently sending requests to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa50559d-8b92-4fd3-82f5-5831ff88f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to create and send the request to the database\n",
    "def load_raw(params):\n",
    "    data_selec=params[\"extract_method\"]\n",
    "    if data_selec==\"Random\":\n",
    "        statement=create_sql_request_random(params)\n",
    "        print(statement)\n",
    "    if data_selec==\"IPC\":\n",
    "        statement=create_sql_request_ipc(params)\n",
    "    if data_selec==\"Date\":\n",
    "        statement=create_sql_request_date(params)\n",
    "    results = epab.sql_query(statement) \n",
    "    log(\"finished loading\")\n",
    "    #TODO add CPC and other interesting metrics parameters\n",
    "    docs = [{'id': item['epab_doc_id'], 'original_text': item['description'], 'pubdate':item['pubdate'],'pubnbr':item['pubnbr']} for item in results]\n",
    "    save_list(docs,params[\"filename\"])\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9047256-63ee-4e96-8b9e-bdbf8dd05a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf57a107a474faba065717c009e6c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='File Name:', placeholder='Enter file name', style=TextStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a style with a wider description width\n",
    "style = {'description_width': '150px'}  # Increase as needed\n",
    "\n",
    "# Create form widgets with the updated style\n",
    "file_name_widget = widgets.Text(\n",
    "    value='',\n",
    "    description='File Name:',\n",
    "    placeholder='Enter file name',\n",
    "    style=style\n",
    ")\n",
    "\n",
    "number_widget = widgets.IntText(\n",
    "    value=10,\n",
    "    description='Number of patents:',\n",
    "    style=style\n",
    ")\n",
    "\n",
    "numberextract_widget = widgets.IntText(\n",
    "    value=5000,\n",
    "    description='Number of characters:',\n",
    "    style=style\n",
    ")\n",
    "\n",
    "extraction_widget = widgets.Dropdown(\n",
    "    options=[\"Random\", \"IPC\", \"Length\", \"Asian\", \"Date\"],\n",
    "    description='Extraction method:',\n",
    "    style=style\n",
    ")\n",
    "\n",
    "data_widget=widgets.Output()\n",
    "\n",
    "start_button = widgets.Button(\n",
    "    description='Start',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "# Define the function to be executed when the button is clicked\n",
    "def on_start_clicked(b):\n",
    "    params = {\n",
    "        \"filename\": file_name_widget.value,\n",
    "        \"nbr\": number_widget.value,\n",
    "        \"nbrextract\": numberextract_widget.value,\n",
    "        \"extract_method\": extraction_widget.value\n",
    "    }\n",
    "    save_data(params)\n",
    "    \n",
    "    docs=load_raw(params)\n",
    "    docs=pd.DataFrame(docs)\n",
    "    with data_widget:\n",
    "        display(docs)\n",
    "    log(\"finished loading patents\")\n",
    "\n",
    "start_button.on_click(on_start_clicked)\n",
    "\n",
    "# Display the form\n",
    "\n",
    "form_items = widgets.VBox([\n",
    "    file_name_widget,\n",
    "    number_widget,\n",
    "    numberextract_widget,\n",
    "    extraction_widget,\n",
    "    start_button,\n",
    "    data_widget\n",
    "])\n",
    "\n",
    "display(form_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9d076f-6069-417f-868d-29e6dfe52754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search for in headings (allowing fuzzy matching with up to one error)\n",
    "keyword1 = [\"background\",  \"prior art\", \"state of the art\", \"field of the invention\", \"technical field\",\"summary\",\"industrial applicability\"]\n",
    "keyword2=[\"background\",\"herein described subject matter\", \"technology described herein\", \"subject of the invention\", \"belongs to the field\", \"invention is\",\"invention relates to\", \"present invention refers to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc9eae-9db4-4d74-8d6d-66f52f5387f8",
   "metadata": {},
   "source": [
    "New code below for extraction of text based on sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb5c33c6-bbb5-471c-ac89-42255e9d0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_sentences(s):\n",
    "    if s.endswith('.'):\n",
    "        # Remove all content in parentheses or brackets, including the symbols\n",
    "        s = re.sub(r'\\([^)]*\\)', '', s)  # remove ( ... )\n",
    "        s = re.sub(r'\\[[^\\]]*\\]', '', s)  # remove [ ... ]\n",
    "\n",
    "        # Remove HTML tags\n",
    "        s = re.sub(r'<[^>]+>', '', s)\n",
    "\n",
    "        # Collapse multiple spaces and strip again\n",
    "        s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def extract_text(text, keyword1, keyword2, min_sentence_length=5):\n",
    "    \"\"\"\n",
    "    Extracts text segments that start with <heading ...> containing keyword1,\n",
    "    and go until the next <heading ...> tag.\n",
    "    If none found, extracts paragraphs containing keyword2 and the next paragraph.\n",
    "    Returns a list of cleaned sentences.\n",
    "    \"\"\"\n",
    "    # Pattern to match headings and content following them\n",
    "    pattern = re.compile(r'(<heading[^>]*?>)(.*?)</heading>(.*?)(?=<heading|$)', re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    matches = pattern.findall(text)\n",
    "\n",
    "    extracted_segments = []\n",
    "    for full_tag, heading_text, following_text in matches:\n",
    "        # Check if heading text contains any keyword1\n",
    "        if any(kw.lower() in heading_text.lower() for kw in keyword1):\n",
    "            extracted_segments.append(following_text.strip())\n",
    "\n",
    "    if extracted_segments:\n",
    "        combined_text = \"\\n\".join(extracted_segments)\n",
    "    else:\n",
    "        # Fallback: extract paragraphs based on keyword2\n",
    "        paragraphs = text.split(\"</p>\")\n",
    "        extracted_paragraphs = []\n",
    "        for i, para in enumerate(paragraphs):\n",
    "            if any(kw.lower() in para.lower() for kw in keyword2):\n",
    "                extracted_paragraphs.append(para)\n",
    "                if i + 1 < len(paragraphs):\n",
    "                    extracted_paragraphs.append(paragraphs[i + 1])\n",
    "        combined_text = \"\\n\\n\".join(extracted_paragraphs)\n",
    "\n",
    "    # Split into sentences using simple punctuation rule\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', combined_text)\n",
    "    sentences = [s.strip() for s in sentences if (len(s.split()) >= min_sentence_length)]\n",
    "    sentences=[clean_sentences(s) for s in sentences if s.endswith('.')]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def process_texts(texts, keyword1, keyword2, min_sentence_length=5):\n",
    "    \"\"\"\n",
    "    Processes a list of texts, each with an associated id.\n",
    "    \n",
    "    Args:\n",
    "        texts (list of tuples): Each tuple is (text_id, text).\n",
    "        keyword1 (list): Keywords for heading-based extraction.\n",
    "        keyword2 (list): Fallback keywords for paragraph-based extraction.\n",
    "        min_sentence_length (int): Minimum number of words a sentence must have.\n",
    "        \n",
    "    Returns:\n",
    "        list of tuples: Each tuple is (text_id, sentence) for each extracted sentence.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for item in texts:\n",
    "        sentences = extract_text(item[\"original_text\"], keyword1, keyword2, min_sentence_length)\n",
    "        if len(sentences)==0:\n",
    "            sentences=[\"\"]\n",
    "        for sentence in sentences:\n",
    "            results.append({\"id\":item[\"id\"], \"sentence\":sentence})\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def merge_sentence(processed_texts):\n",
    "    \"\"\"\n",
    "    Merges sentences by text_id from the processed texts.\n",
    "    \n",
    "    Args:\n",
    "        processed_texts (list of tuples): Each tuple is (text_id, sentence).\n",
    "        \n",
    "    Returns:\n",
    "        list of dict: Each dictionary has the text_id as key and the merged sentences as value.\n",
    "    \"\"\"\n",
    "    merged = {}\n",
    "    for item in processed_texts:\n",
    "        text_id=item[\"id\"]\n",
    "        sentence=item[\"sentence\"]\n",
    "        if text_id not in merged:\n",
    "            merged[text_id] = sentence\n",
    "        else:\n",
    "            merged[text_id] += \"\\n\" + sentence\n",
    "    # Convert to list of dictionaries as required.\n",
    "    return [{\"id\":text_id,\"text\":sentences, \"status\":\"\"} for text_id, sentences in merged.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61ec4736-dc0d-4141-83c8-f439aab1ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def compute_sentence_stats(df, plot_widget):\n",
    "    \"\"\"\n",
    "    Computes statistics on the number of sentences per text_id.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame with columns 'text_id' and 'sentence'\n",
    "    \n",
    "    Returns:\n",
    "        stats (dict): Dictionary containing the following keys:\n",
    "                      - 'mean': average number of sentences per text_id\n",
    "                      - 'min': minimum number of sentences in any text_id\n",
    "                      - 'max': maximum number of sentences in any text_id\n",
    "                      - 'square_mean': mean of the squared sentence counts per text_id\n",
    "    \"\"\"\n",
    "    # Group by text_id and count the sentences per group\n",
    "    sentence_counts = df.groupby('id')['sentence'].count()\n",
    "    \n",
    "    # Compute the required statistics\n",
    "    mean_sentences = sentence_counts.mean()\n",
    "    min_sentences = sentence_counts.min()\n",
    "    max_sentences = sentence_counts.max()\n",
    "    square_mean_sentences = np.mean(sentence_counts**2)\n",
    "    \n",
    "    # Prepare the results in a dictionary\n",
    "    stats = {\n",
    "        'mean': mean_sentences,\n",
    "        'min': min_sentences,\n",
    "        'max': max_sentences,\n",
    "        'square_mean': square_mean_sentences\n",
    "    }\n",
    "\n",
    "    with plot_widget:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        # Plot the distribution (histogram) of sentence counts per text_id\n",
    "        plt.hist(sentence_counts, bins=20, edgecolor='black')\n",
    "        plt.title(\"Distribution of Sentence Counts per text_id\")\n",
    "        plt.xlabel(\"Number of Sentences\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "    \n",
    "    return stats, plot_widget\n",
    "\n",
    "def analyze_sentence_data(data, plot_widget):\n",
    "    return compute_sentence_stats(pd.DataFrame(data), plot_widget)    \n",
    "\n",
    "def analyze_text_data(data,plot_widget):\n",
    "    \"\"\"\n",
    "    Analyzes a list of dictionaries containing text data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (list): A list where each element is a dictionary with an 'id'\n",
    "                     and a nested dictionary under 'data' that contains a \n",
    "                     'sentence' and 'status'.\n",
    "                     \n",
    "    Returns:\n",
    "        stats (dict): A dictionary containing the mean, square-mean, min, and max word counts.\n",
    "        plot_widget (ipywidgets.Output): An ipywidget containing a histogram of the word counts.\n",
    "    \"\"\"\n",
    "    word_counts = []\n",
    "    nbr=0\n",
    "    for entry in data:\n",
    "        # Adjust the keys if your structure is different.\n",
    "        nbr+=1\n",
    "        sentence = entry['text']\n",
    "        count = len(sentence.split())\n",
    "        word_counts.append(count)\n",
    "    word_counts = np.array(word_counts)\n",
    "    \n",
    "    # Compute statistics\n",
    "    stats = {\n",
    "        'Nbr of entries': nbr,\n",
    "        'mean': word_counts.mean(),\n",
    "        'square_mean': np.mean(word_counts**2),\n",
    "        'min': word_counts.min(),\n",
    "        'max': word_counts.max()\n",
    "    }\n",
    "    \n",
    "    # Create an ipywidget Output for the plot\n",
    "    \n",
    "    with plot_widget:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(word_counts, bins=20, edgecolor='black')\n",
    "        plt.title(\"Distribution of Word Counts\")\n",
    "        plt.xlabel(\"Number of Words\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "        \n",
    "    return stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34756c91-8a82-423a-b6f5-fe2e5bdbf8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_id(list1, list2):\n",
    "    # Create a lookup dictionary from list2 using 'id' as the key\n",
    "    lookup = {item['id']: item['original_text'] for item in list2}\n",
    "    lookup1 = {item['id']: item['pubnbr'] for item in list2}\n",
    "    lookup2 = {item['id']: item['pubdate'] for item in list2}\n",
    "\n",
    "    # Merge with corresponding entry in list1\n",
    "    merged = []\n",
    "    for item in list1:\n",
    "        merged_item = {\n",
    "            'id': item['id'],\n",
    "            'text': item['text'],\n",
    "            'pubdate': lookup2.get(item['id']) ,\n",
    "            'pubnbr':lookup1.get(item['id']) ,\n",
    "            'original_text': lookup.get(item['id'])  # Use .get() to avoid KeyError\n",
    "        }\n",
    "        merged.append(merged_item)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d509318-bae7-4021-9a35-ff7e421d76ba",
   "metadata": {},
   "source": [
    "## DATA extraction\n",
    "\n",
    "This is the configuration file to define the extraction process. The extraction parameters are saved in a \".ext\" file and the result of the extraction in \".dat.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ffdc6fa-3ee8-4b8e-bb41-a8377b3bf972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2eeae1d7d904b1a9387b631a4d95ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Button(description='Update', style=ButtonStyle()), Select(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create the left side: Update button and file list\n",
    "update_button = widgets.Button(description=\"Update\")\n",
    "file_list = widgets.Select(options=[], rows=10, description=\"Files:\")\n",
    "\n",
    "# Create the right side: Inputs for filename, raw source, comment, and cut-off length.\n",
    "filename_input = widgets.Text(description=\"Filename:\")\n",
    "raw_source_input = widgets.Text(description=\"Raw source file:\")\n",
    "comment_input = widgets.Textarea(\n",
    "    description=\"Comment:\",\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "select_button = widgets.Button(description=\"Extract\")\n",
    "\n",
    "def update_file_list(button):\n",
    "    \"\"\"\n",
    "    Updates the file list with files ending in '.ext' in the current directory.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir('.') if f.endswith('.ext')]\n",
    "    file_list.options = files\n",
    "\n",
    "def load_file_content(change):\n",
    "    \"\"\"\n",
    "    Loads the content of the selected file and populates the input widgets.\n",
    "\n",
    "    Args:\n",
    "        change (dict): The change dictionary from the file_list.observe event.\n",
    "    \"\"\"\n",
    "    selected_file = change.new\n",
    "    if selected_file:\n",
    "        try:\n",
    "            with open(selected_file, 'r') as f:\n",
    "                file_content = f.read()\n",
    "                # Try to parse as JSON first, if it fails, treat as raw text\n",
    "                try:\n",
    "                    data = json.loads(file_content)\n",
    "                    filename_input.value = data.get(\"filename\", \"\")\n",
    "                    raw_source_input.value = data.get(\"raw_source_file\", \"\")\n",
    "                    comment_input.value = data.get(\"comment\", \"\")\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    # If it's not JSON, put the whole content into raw_source_input\n",
    "                    raw_source_input.value = file_content\n",
    "                    filename_input.value = os.path.splitext(selected_file)[0]  # Set filename without extension\n",
    "                    comment_input.value = \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file: {e}\")\n",
    "            filename_input.value = \"\"\n",
    "            raw_source_input.value = \"\"\n",
    "            comment_input.value = \"\"\n",
    "\n",
    "def on_select(button):\n",
    "    \"\"\"\n",
    "    Gathers parameters, determines the output filename, saves parameters as JSON,\n",
    "    and calls the process_texts function (which is assumed to be defined elsewhere).\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"filename\": filename_input.value,\n",
    "        \"raw_source_file\": raw_source_input.value,\n",
    "        \"comment\": comment_input.value,\n",
    "        \"selected_file\": file_list.value\n",
    "    }\n",
    "\n",
    "    save_filename = filename_input.value\n",
    "    if not save_filename.endswith(\".ext\"):\n",
    "        save_filename = save_filename + \".ext\"\n",
    "\n",
    "    try:\n",
    "        with open(save_filename, \"w\") as f:\n",
    "            json.dump(params, f, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "    texts=load_list(params[\"raw_source_file\"])\n",
    "    extracted=process_texts(texts, keyword1, keyword2)\n",
    "    metrics0=analyze_sentence_data(extracted,plot_widget0)\n",
    "    merged=merge_sentence(extracted)\n",
    "    result=merge_by_id(merged,texts)\n",
    "    metrics=analyze_text_data(merged,plot_widget)\n",
    "    log2(str(metrics0))\n",
    "    save_list(extracted,params[\"filename\"]+\".sen.gz\")\n",
    "    save_list(result,params[\"filename\"]+\".dat.gz\")\n",
    "    metrics[\"filename\"]=params[\"filename\"]+\".met\"\n",
    "    #save_data(metrics)\n",
    "    log2(str(metrics))  \n",
    "    log2(\"finished\")\n",
    "\n",
    "# Set up event handlers\n",
    "update_button.on_click(update_file_list)\n",
    "file_list.observe(load_file_content, names='value')  # Observe changes to file_list.value\n",
    "select_button.on_click(on_select)\n",
    "\n",
    "# Organize the layout with two columns.\n",
    "left_box = widgets.VBox([update_button, file_list])\n",
    "right_box = widgets.VBox([\n",
    "    filename_input,\n",
    "    raw_source_input,\n",
    "    comment_input,\n",
    "    select_button\n",
    "])\n",
    "ui1 = widgets.HBox([left_box, right_box])\n",
    "log_output2 = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Results will appear here.',\n",
    "    description='Logs:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%', height='100px')  # Adjust size as needed\n",
    ")\n",
    "plot_widget = widgets.Output()\n",
    "plot_widget0 = widgets.Output()\n",
    "log_output2.value=\"\"\n",
    "ui=widgets.VBox([ui1,log_output2,plot_widget0,plot_widget])\n",
    "\n",
    "# Display the UI in the notebook.\n",
    "display(ui)\n",
    "\n",
    "def log2(text):\n",
    "    \"\"\"\n",
    "    Adds text to the log display widget.\n",
    "    \"\"\"\n",
    "    log_output2.value += text + '\\n'  # Append the new text with a newline character\n",
    "    \n",
    "# Initial update of the file list\n",
    "update_file_list(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c850a489-83f4-4069-b177-6bf675efe34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fails(data):\n",
    "    return [item for item in data if len(item.get('extracted_text', '')) < 50 or len(item.get('extracted_text', '')) > 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfe139fd-a2fb-4e8f-afdc-266846259ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def html_to_text(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # The separator=\"\\n\" inserts a newline between blocks of text\n",
    "    return soup.get_text(separator=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fc9f8fb-4e3b-4648-9e69-aeff25b16ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9058d8b801d44ec9d1d996215e6f05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Select(description='Files:', layout=Layout(width='200px'), option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def filter_fails(data):\n",
    "    return [item for item in data if item.get('difficult')]\n",
    "\n",
    "# Get list of files ending with \".dat.gz\" from current directory\n",
    "files = [f for f in os.listdir('.') if f.endswith('dat.gz')]\n",
    "\n",
    "# Widget for listing files\n",
    "file_selector = widgets.Select(\n",
    "    options=files,\n",
    "    description='Files:',\n",
    "    rows=10,\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "patentnbr_input = widgets.Text(description=\"Patent number:\")\n",
    "patentdate_input = widgets.Text(description=\"Patent date:\")\n",
    "\n",
    "status_dropdown = widgets.Dropdown(\n",
    "    options=['OK', 'WRONG', 'NONE'],\n",
    "    value='NONE',\n",
    "    description='Status:'\n",
    ")\n",
    "\n",
    "# Text area to display the contents of the corresponding \".met.conf\" file\n",
    "met_conf_text = widgets.Textarea(\n",
    "    description='Config:',\n",
    "    layout=widgets.Layout(width='500px', height='200px')\n",
    ")\n",
    "\n",
    "save_button = widgets.Button(description=\"SAVE\")\n",
    "\n",
    "# Callback to load and display the .met.conf file content when a file is selected\n",
    "def on_file_select(change):\n",
    "    selected_file = file_selector.value\n",
    "    if selected_file:\n",
    "        met_conf_filename = selected_file.replace('.dat.gz', '.ext')\n",
    "        if os.path.exists(met_conf_filename):\n",
    "            with open(met_conf_filename, 'r') as f:\n",
    "                content = f.read()\n",
    "        else:\n",
    "            content = f\"File not found: {met_conf_filename}\"\n",
    "        met_conf_text.value = str(content)\n",
    "\n",
    "file_selector.observe(on_file_select, names='value')\n",
    "\n",
    "# Button to load the list (using load_list function)\n",
    "load_button = widgets.Button(description='Load')\n",
    "list_data = []  # Will store the list of dictionaries loaded by load_list\n",
    "filtered_data = []\n",
    "current_index = 0  # To keep track of the current row\n",
    "\n",
    "# Checkbox to toggle difficult-only filtering\n",
    "difficult_only_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Display difficult files only'\n",
    ")\n",
    "\n",
    "# Text areas for displaying the extracted_text and original_text fields\n",
    "extracted_text_area = widgets.Textarea(\n",
    "    description='Extracted:',\n",
    "    layout=widgets.Layout(width='500px', height='200px')\n",
    ")\n",
    "original_text_area = widgets.Textarea(\n",
    "    description='Original:',\n",
    "    layout=widgets.Layout(width='500px', height='200px')\n",
    ")\n",
    "\n",
    "def get_display_data():\n",
    "    return filter_fails(list_data) if difficult_only_checkbox.value else list_data\n",
    "\n",
    "# Update the text areas based on the current index\n",
    "def update_text_areas():\n",
    "    display_data = get_display_data()\n",
    "    if display_data and 0 <= current_index < len(display_data):\n",
    "        extracted_text_area.value = display_data[current_index].get('text', '')\n",
    "        original_text_area.value = display_data[current_index].get('original_text', '')\n",
    "        patentnbr_input.value=display_data[current_index].get('pubnbr')\n",
    "        patentdate_input.value=display_data[current_index].get('pubdate')\n",
    "        status_dropdown.value = display_data[current_index].get('status', 'NONE')\n",
    "    else:\n",
    "        extracted_text_area.value = ''\n",
    "        original_text_area.value = ''\n",
    "        patentnbr_input.value=''\n",
    "        patentdate_input.value=''\n",
    "\n",
    "def on_load_button_click(b):\n",
    "    global list_data, current_index\n",
    "    if file_selector.value:\n",
    "        selected_file=file_selector.value\n",
    "        list_data = load_list(file_selector.value)\n",
    "        current_index = 0\n",
    "        update_text_areas()\n",
    "        met_conf_filename = selected_file.replace('.dat.gz', '.ext')\n",
    "        if os.path.exists(met_conf_filename):\n",
    "            with open(met_conf_filename, 'r') as f:\n",
    "                content = f.read()\n",
    "        else:\n",
    "            content = f\"File not found: {met_conf_filename}\"\n",
    "        met_conf_text.value = str(content)\n",
    "    else:\n",
    "        extracted_text_area.value = 'No file selected'\n",
    "        original_text_area.value = ''\n",
    "\n",
    "def on_status_change(change):\n",
    "    # Ensure we're only handling a value change event\n",
    "    if change['name'] == 'value' and change['type'] == 'change':\n",
    "        display_data = get_display_data()\n",
    "        if display_data and 0 <= current_index < len(display_data):\n",
    "            display_data[current_index]['status'] = change['new']\n",
    "\n",
    "status_dropdown.observe(on_status_change, names='value')\n",
    "\n",
    "load_button.on_click(on_load_button_click)\n",
    "difficult_only_checkbox.observe(lambda change: update_text_areas(), names='value')\n",
    "\n",
    "# Navigation buttons for moving through rows\n",
    "left_arrow = widgets.Button(description='<')\n",
    "right_arrow = widgets.Button(description='>')\n",
    "\n",
    "def on_left_arrow_click(b):\n",
    "    global current_index\n",
    "    if current_index > 0:\n",
    "        current_index -= 1\n",
    "        update_text_areas()\n",
    "\n",
    "def on_right_arrow_click(b):\n",
    "    global current_index\n",
    "    if current_index < len(get_display_data()) - 1:\n",
    "        current_index += 1\n",
    "        update_text_areas()\n",
    "\n",
    "left_arrow.on_click(on_left_arrow_click)\n",
    "right_arrow.on_click(on_right_arrow_click)\n",
    "\n",
    "# Define the button's click event handler\n",
    "def on_save_click(b):\n",
    "    save_list(display_data,file_selector.value+\".check\")\n",
    "\n",
    "# Attach the event handler to the button's click event\n",
    "save_button.on_click(on_save_click)\n",
    "\n",
    "# Layout the widgets\n",
    "file_list_box = widgets.VBox([file_selector, load_button])\n",
    "file_and_config = widgets.HBox([file_list_box, met_conf_text])\n",
    "text_areas_box = widgets.HBox([extracted_text_area, original_text_area])\n",
    "nav_buttons = widgets.HBox([left_arrow, right_arrow])\n",
    "\n",
    "# Combine all into one UI layout\n",
    "ui = widgets.VBox([file_and_config, difficult_only_checkbox, patentdate_input, patentnbr_input,    status_dropdown, text_areas_box, nav_buttons,save_button])\n",
    "display(ui)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
